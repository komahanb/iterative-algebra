\documentclass{article}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{fancyvrb}
\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ : \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework\ \#1}
\newcommand{\hmwkDueDate}{\today}
\newcommand{\hmwkClass}{MATH6644 Iterative Methods}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{}
\newcommand{\hmwkAuthorName}{Komahan Boopathy}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{\hmwkDueDate}\\
    %\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\Large\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}
\maketitle
\thispagestyle{empty}

\pagebreak

\begin{homeworkProblem}
  Find an orthonormal basis for the column space of matrix
  $$A = 
  \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 0 & 2 \\
    1 & 0 & 1 \\
    1 & 1 & -1
  \end{bmatrix}$$

  The columns $\mathbf{c_1}$, $\mathbf{c_1}$ and $\mathbf{c_3}$ are
  \underline{not orthonormal} to each other. Gram-Scmidt orthogonalization
  procedure is used to orthogonalize the columns as follows. 
  \\
  The first
  column $\mathbf{c_1}$ is chosen as the reference. 
  $$\mathbf{v_1} = \mathbf{c_1} = 
  \begin{bmatrix}
    1 \\
    1 \\
    1 \\
    1
  \end{bmatrix}$$

  The second vector
  \begin{equation*}
    \begin{split}
    \mathbf{v_2} & = \mathbf{c_2} - \dfrac{(\mathbf{c_2},\mathbf{v_1})}{(\mathbf{v_1},\mathbf{v_1})}\mathbf{v_1} \\
    & =   \begin{bmatrix}
    1 \\
    0 \\
    0 \\
    1
  \end{bmatrix}
    -\dfrac{1}{2}
    \begin{bmatrix}
      1 \\
      1 \\
      1 \\
      1
    \end{bmatrix}
     = 
    \begin{bmatrix}
      1/2 \\
      -1/2 \\
      -1/2 \\
      1/2
    \end{bmatrix}
    \end{split}
  \end{equation*}

  The third vector
    \begin{equation*}
  \begin{split}
      \mathbf{v_3}  & = \mathbf{c_3} - \dfrac{(\mathbf{c_3},\mathbf{v_1})}{(\mathbf{v_1},\mathbf{v_1})}\mathbf{v_1} - \dfrac{(\mathbf{c_3},\mathbf{v_2})}{(\mathbf{v_2},\mathbf{v_2})}\mathbf{v_2} \\
      & =
      \begin{bmatrix}
        0 \\
        2 \\
        1 \\
        -1
      \end{bmatrix}
      -\dfrac{1}{2}
      \begin{bmatrix}
        1 \\
        1 \\
        1 \\
        1
      \end{bmatrix}
      +\dfrac{2}{1}
      \begin{bmatrix}
      1/2 \\
      -1/2 \\
      -1/2 \\
      1/2
      \end{bmatrix}
     = 
     \begin{bmatrix}
       1/2 \\
       1/2 \\
       -1/2 \\
       -1/2
    \end{bmatrix}
    \end{split}
  \end{equation*}

  Finally these vectors are \underline{normalized to unity} and the corresponding
  orthonormal basis for the column space of $\mathbf{A}$ is
  $$\begin{bmatrix}
    1/2 &  1/2 & 1/2  \\
    1/2 & -1/2 & 1/2  \\
    1/2 & -1/2 & -1/2 \\
    1/2 &  1/2 & -1/2
  \end{bmatrix}$$

\end{homeworkProblem}

\clearpage
\begin{homeworkProblem}
  Consider a matrix
  $$A = 
  \begin{bmatrix}
     2 & 2 & -2 \\
     2 & 6 & 0 \\
    -2 & 0 & 7 
  \end{bmatrix}$$
 
  \paragraph{a.} 
  \emph{Is this matrix diagonalizable? Why or why not?}
  \medskip
  A matrix is diagonalizable if there are $n$ linearly independent
  eigenvectors. Then, we can do a similarity transformation of the
  form $J=V^{-1}AV$, where $V$ is the matrix of eigenvectors.  
  \\ 
  
  The
  eigenvalues are found using $|A-\lambda I|=0$ which results in the
  cubic characteristic equation
  $$
  -\lambda^3 + 15\lambda^2 -60\lambda + 32 = 0
  $$ 

  The eigenvalues are $\lambda_1 = 8$, $\lambda_2 =
  \frac{7+\sqrt{33}}{2}$ and $\lambda_2 = \frac{7-\sqrt{33}}{2}$ The
  eigenvalues are distinct (not repeated), therefore we can find
  eigenvectors that form a simularity transformation as shown
  above. Therefore it is possible to diagonalize the matrix.

  \paragraph{b.}
  \emph{Compute the 1-norm, 2-norm, Frobenius norm and infinite-norm.}
  \medskip

  \begin{itemize}
  \item  1-norm of the matrix is the largest column sum i.e. $\max
    \{\sum{c_1}, \sum{c_2}, \sum{c_3}\} = \max\{6,8,9\}=9$

  \item  2-norm of the matrix is the largest singular value (eigenvalue
    in this case) i.e., $\max \{8,\frac{7-\sqrt{33}}{2},\frac{7-\sqrt{33}}{2}\} = 8$.

  \item  Infinite-norm of the matrix is the largest row sum i.e., $\max
    \{\sum{r_1}, \sum{r_2}, \sum{r_3}\} = \max\{8,8,9\}=9$

  \item Frobenius-norm of the matrix is $\sum_{i,j} |A_{ij}|^2 = \sqrt{2^2+2^2+2^2+2^2+6^2+2^2+7^2} = 10.247$.

  \end{itemize}

  \paragraph{c.} 
  \emph{Compute the spectral radius of A. Compare and comment on the values from (b) and (c).}
  \medskip

  The spectral radius of A is $\rho(A) = \max\{\lambda_i\} = 8$.  In
  this case, the spectral radius is same as the 2-norm of the matrix
  A. The 2-norm of the matrix is the smallest norm of the different
  computed norms of the matrix.

\end{homeworkProblem}
\clearpage
\begin{homeworkProblem}
  \emph{Give the matrix expression for the symmetric Gauss-Seidel iterations.}
  \medskip

  The matrix $A = D-E-F$ and the right hand side $b$. The symmetric
  Gauss-Seidel iterations are defined as follows. 
  One iteration of Symmetric GS involves one forward and one reverse
  sweeps, each of ${\cal{O}}(n^2)$ operations.


  \paragraph{Forward Sweep}

  $$(D-E) x^{k+1/2} = F x^{k} + b$$

  The strictly lower triangular system D-E is swept using forward
  substitution method.
  
  \paragraph{Backward Substitution}

  $$(D-F) x^{k+1} = E x^{k+1/2} + b$$

  The strictly upper triangular system D-E is swept using backward
  substitution method.

\end{homeworkProblem}
\clearpage
\begin{homeworkProblem}
Construct a $3 \times 3$ dense matrix with $\rho(A)>1$. Define an
iteration,
$$
\mathbf{x}^{k+1} = G \mathbf{x} + \mathbf{c}.
$$ 
Find an example $\mathbf{x_0}$ and $\mathbf{c}$ such that the
iteration does not converge.
\medskip

\paragraph{Example}

Let the matrix $A = M-N$.  We define an iteration,
$$
\mathbf{x}^{k+1} = G \mathbf{x} + \mathbf{c}.
$$ where $G = M^{-1}N$ and $c = M^{-1}b$. 
%Consider a Gauss-Jacobi
%iteration where $G=I-D^{-1}A$, where $D$ being the diagonal entries of
%the matrix A.
Let
$$\mathbf{G} = 
\begin{bmatrix}
  0.1 & 1 & 1 \\
  1 & 0.1 & 1 \\
  1 & 1 & 0.1
\end{bmatrix}$$
$$
\mathbf{c} = 
  \begin{bmatrix}
    1 \\
    1 \\
    1 \\
  \end{bmatrix}$$
and 
$$
\mathbf{x_0} = 
  \begin{bmatrix}
    1 \\
    2 \\
    3 \\
  \end{bmatrix}$$
The following sequence of solutions is obtained for iterations
\begin{verbatim}
1    6.1000    5.2000    4.3000
2   11.1100   11.9200   12.7300
3   26.7610   26.0320   25.3030
4   55.0111   55.6672   56.3233
5  118.4916  117.9011  117.3106
\end{verbatim}

\paragraph{Discussion}

It can be seen that the scheme diverges and reaches infinity within a
hundred iterations. The spectral radius of $\mathbf{G}$ is 2.1 which
is greater than 1 and therefore we can not expect this iteration to
converge.  An iterative method is guaranteed to converge if $\rho(G) <
1$. On the contrary, in this case the spectral radius is greater than
1 and therefore the iterative method will not converge. The
convergence is largly not caused by the choice of the initial guess
$x_0$ , but rather is a properly of the iteration matrix, because it
is the matrix which enlarges (or diminishes) the solution vector
repeatedly (unless it is an eigenvector at that iteration). Only, the
\emph{rate} of convergence is will be affected by $x_0$. Also note
that the vector $\mathbf{c}$ \emph{simply} translates the solution and
would not scale the solution up or down.

\end{homeworkProblem}
\clearpage
\begin{homeworkProblem}
\emph{Discretize the following differential equation}
  $$ -u^{\prime\prime} + 4 u = 0 $$
  $$ u(0) = -1, u(1) = 2, x\in [0,1]$$ 
\emph{by the central
    difference scheme. Write your linear system of equaitons. Solve
    the system by classical iterations such as Jacobi, Gauss-Seidel,
    SOR with n = 1000. Test the relaxation parameter for several
    values and decide which one is better.  You need to discuss your
    results. }

  \paragraph{a. Discretization of ODE}
  \medskip
  The discretized ODE is  $ - \dfrac{u_{i+1} -2 u_i + u_{i-1}}{h^2} + 4 u_i = 0.$
  This can be rewritten as follows$$- u_{i+1} + (2 + 4h^2) u_i - u_{i-1}  = 0.$$
 
  \paragraph{b. Linear system of equations}
  \medskip
  The matrix 
  $$\mathbf{A} = 
  \begin{bmatrix}
    (2 + 4h^2) & -1         & 0 & 0 \\
    -1         & (2 + 4h^2) & -1 & 0  \\
    0          & -1         & (2 + 4h^2) & -1  \\
    \vdots      &      & \ddots  &   \\
    0          &       0    & -1        & (2 + 4h^2)
  \end{bmatrix}$$
This is a banded tri-diagonal system. The right hand side is
 $$\mathbf{b} = 
  \begin{bmatrix}
    -1 \\
    0 \\
    0 \\
    \vdots\\
    2
  \end{bmatrix}$$
  There are 1000 unknowns in this problem.

  \paragraph{c. Iterative solutions}
  The linear system is solved using Gauss-Jacobi, Gauss-Seidel and SOR
  methods. The stopping tolerance was taken to be $10^{-6}$ and
  maximum allowed iterations are $10^{5}$. The exact solution is
  computed using direct factorization method.  Figure~\ref{fig:profile} compares
  the solutions produced by each of these methods. All methods were
  converged to the same tolerance and therefore produce similar
  profile for the unknown $u(x)$. A minor discrepancy with the exact
  solution can also be noticed but it can be reduced with a higher
  tolerance for convergence.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{profile.pdf}    
    \caption{Plot of solution profile in the domain for different
      solution methods.}
    \label{fig:profile}
  \end{figure}

  %plot of u vs. x for 1,2, 3, 4

  \paragraph{d. Relaxation Parameter}

  Several values of the relaxation parameter $\omega$ was tested
  between 0.4 and 2.0. Figure~\ref{fig:relaxation} shows the plot of the value of
  the relaxation parameter with the number of SOR iteration took to
  converge to the specified tolerance. From this plot it can be seen
  that the optimal value of $\omega$ lies somewhere between in (1.99,
  2.0). As soon as $\omega$ hits 2.0, the speedup is lost and also SOR
  becomes unstable.

  % plot of omega vs. niters for sor

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{relaxation_study.pdf}    
    \caption{Finding optimal relaxation parameter based on
      experimentation.}
    \label{fig:relaxation}
  \end{figure}

  The best perfoming SOR with $\omega=1.99$ is compared with plain
  Gauss-Jacobi and Gauss-Seidel in Figure~\ref{fig:convergence} in
  terms of the number of iterations taken to converge to a
  solution. Gauss Seidel is faster than Gauss Jacobi. SOR is faster
  than the other two methods.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{convergence.pdf}    
    \caption{Convergence of iterative solution methods.}
    \label{fig:convergence}
  \end{figure}

  % plot of tol vs. niters for 1, 2, 3
  
  In general, the number of iterations are too many for the problem
  size of $1000\times1000$ for GJ and GS methods and SOR without
  optimal $\omega$. The reason is that as $h \to 0$ (with refinement
  of the discretization), the matrix $A$ loses its diagonal
  dominance. Mathematically,
  $$ \lim_{h \to 0} 2 + 4h^2 \approx 2 $$ which equals the sum of
  magnitudes of the non-diagonal entries (-1 and -1 in this case).
  This can be seen as the reason for poor performance in this test
  case.  Therefore proper care must be exercised before choosing h and
  order of discretization of derivatives for the iterative schemes to
  perform well.

\end{homeworkProblem}


\end{document}
