\documentclass{article}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage{booktabs}
\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ : \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework\ \#2}
\newcommand{\hmwkDueDate}{\today}
\newcommand{\hmwkClass}{MATH6644 Iterative Methods}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{}
\newcommand{\hmwkAuthorName}{Komahan Boopathy}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{\hmwkDueDate}\\
    %\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\Large\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}
\maketitle
\thispagestyle{empty}

\pagebreak

\begin{homeworkProblem}
Consider a matrix
$$A = 
\begin{bmatrix}
  6 & -1 & 0 \\ -1 & 5 & 0 \\ 0 & 0 & 2 \\
\end{bmatrix}$$

\paragraph{a.} \emph{Show that A is positive definite.}
\medskip

The eigenvalues of the matrix $A$ is found using $|A-\lambda
I|=0$. The characterisitic polynomial is $(\lambda^2 -11\lambda +
29)(\lambda-2)=0$. The eigenvalues are $\lambda=2,
\frac{11-\sqrt{5}}{2}, \frac{11+\sqrt{5}}{2}$, all of which are real
(due to symmetry of $A$) and greater than zero (due to positive
definiteness of $A$).

\paragraph{b.} \emph{Calculate the A-norm of the vector.}
$$ \mathbf{b} = 
\begin{bmatrix}
  1 \\ 2 \\ -1
\end{bmatrix} $$
\medskip
The A-norm of vector $||b||_{A} = \sqrt{(b, Ab)}$. We find
$$ \mathbf{c} = \mathbf{Ab} = 
\begin{bmatrix}
  6 & -1 & 0 \\
 -1 &  5 & 0 \\
  0 &  0 & 2 \\
\end{bmatrix} 
\begin{bmatrix}
  1 \\ 2 \\ -1
\end{bmatrix} =
\begin{bmatrix}
  4 \\ 9 \\ -2
\end{bmatrix}
$$
The A-norm of vector $b$ is therefore $\sqrt{(b,c)}=4.899$

\paragraph{c.} \emph{Find a vector that is $A-$orthogonal to $\mathbf{b}$.}

This means that we need to find a vector $\mathbf{d}$ that such that
$(\mathbf{d}, \mathbf{Ab}) = (\mathbf{d}, \mathbf{c}) = 0$. We are
essentially finding a conjugate vector to $\mathbf{b}$. We can use
Gram-Schmidt orthogonalization to get that vector $\mathbf{d}$ that is
orthogonal to $\mathbf{c}$ as follows:
\begin{equation*}
  \begin{split}
    \mathbf{d} & = \mathbf{b} - \frac{(\mathbf{b}, \mathbf{c})}{(\mathbf{c},\mathbf{c})} \mathbf{c} \\
    & = \begin{bmatrix}
      1 \\
      2 \\
      -1
    \end{bmatrix}
    - \dfrac{24}{101}
    \begin{bmatrix}
      4 \\
      9 \\
      -2 
    \end{bmatrix}
    = 
    \begin{bmatrix}
    0.0495\\
   -0.1386\\
   -0.5248
    \end{bmatrix}
  \end{split}
\end{equation*}
This can be normalized if necessary to the following vector 
$$
\mathbf{\hat{d}} = 
\begin{bmatrix}
  0.0908\\
  -0.2543\\
  -0.9628
\end{bmatrix}
$$

\end{homeworkProblem}

\clearpage
\begin{homeworkProblem}
 Assume that $A$ is a $n\times n$ symmetric positive definite
  matrix. If $\kappa(A) = {\cal{O}}(n)$, give a rough estimate of the
  number of steepest descent iterations required to reduce the error
  in $A-norm$ to ${\cal{O}}(1/n)$.

  \medskip

  Let $m$ be the number of steepest descent iterations required to
  reduce the error to ${\cal{O}}(1/n)$ of the initial error. We have
  $$ ||e||_{A}^{m+1} \le \left( \dfrac{\kappa(A)-1}{\kappa(A)+1} \right)^{m+1} ||e||_{A}^{0} $$

  Taking logarithm on both sides:
  $$
  (m + 1) \log \left( \dfrac{\kappa(A)-1}{\kappa(A)+1} \right) = \log \left( {\dfrac{||e||_{A}^{m+1}}{||e||_{A}^{0}}} \right)
  $$
  Using the given estimates for $A$ , this expression becomes
  $$
  (m + 1) \log \dfrac{n-1}{n+1} = \log {\dfrac{1}{n}}
  $$
  On rearranging, we get
$$
  m =  \dfrac{\log {\dfrac{1}{n}}}{\log \dfrac{n-1}{n+1}} - 1
$$ This is the theoretical lower bound for the number of steepest
  descent iterations required to achive the ${\cal{O}}(1/n)$ error
  reduction.
\end{homeworkProblem}

\clearpage
\begin{homeworkProblem}
\emph{Assume that $A$ is a $n\times n$ symmetric positive definite
  matrix with eigenvalues in the intervals $(1, 1.1) \cup (10,10.2)$.
  Assume that the cost of a matrix vector multiplication is about $4n$ floating point
  multiplications. Estimate the number of floating point opearations reduce the 
  $A-$norm of the error  by a factor of $10^{-3}$ using CG iterations. }

  \medskip

  Let $m$ be the number of conjugate gradient iterations required to
  reduce the error to ${\cal{O}}(1/n)$ of the initial error. 
  We have
  $$ ||e||_{A}^{m+1} \le \left( \dfrac{\sqrt{\kappa(A)}-1}{\sqrt{\kappa(A)}+1} \right)^{m+1} ||e||_{A}^{0} $$

  Taking logarithm on both sides:
  $$
  (m + 1) \log \left( \dfrac{\sqrt{\kappa(A)}-1}{\sqrt{\kappa(A)}+1} \right) = 
  \log \left( {\dfrac{||e||_{A}^{m+1}}{||e||_{A}^{0}}} \right)
  $$ 
  The condition number $\kappa(A) =
  \dfrac{|\lambda_{max}|}{|\lambda_{min}|} = \dfrac{10.2}{1} = 10.2$.
  The required relative reduction in error is $10^{-3}$. Using this
  information 
  $$
  m = \dfrac{\log {10^{-3}}}{\log \dfrac{\sqrt{10.2}-1}{\sqrt{10.2}+1}} - 1 = 10.66 \approx 11
  $$ 

  At each CG iteration there are 4 matrix-vector
  multiplications. However, they are not unique operations (rather
  redundant). Therefore, an efficient implementation will require only
  one matrix-vector multiplication per iteration.  It is given that
  each matrix-vector product costs $4n$ floating point
  operations. Thus, the number of floating point operations would be
  $11 \times 4n = 44n$, where $n$ is the size of the matrix.

\end{homeworkProblem}

\clearpage
\begin{homeworkProblem}

\emph{Discretize the following differential equation}
$$ -u^{\prime\prime} = f(x) = 2x -\frac{1}{2}$$
$$u(0) = 1, u(1) = 0,~(case~a)$$ 
$$ u(0) = 1, u^\prime(1) = 0~(case~b)$$ 
$$x \in [0,1]$$ 

\emph{by the central difference scheme with $n$ interior mesh
  points. Solve the resulting linear system by Conjugate Gradient
  (CG), Preconditioned CG with Sine transform $[S]_{j,k} =
  \sqrt{\frac{2}{n+1}}\sin\left(\frac{\pi j k }{n+1}\right)  \quad \forall \quad 1 \le j,k \le
  n$ to construct the preconditioner. Run your code with $n = 64,
  128, 256, 512, 1024, \ldots$.  Compare the iteration numbers used
  for each case in a table. Discuss your own results.}

\cite{sine-transform}

  \paragraph{(a) Linear System}

  \medskip

  The discretized ODE is $ - u_{i+1} +2 u_i - u_{i-1} =
  h^2f(x_i)$. This leads to a tridiagonal banded linear system.

  \medskip

  For the case $(a)$ where both the systems are specified the unknown $u$
  (dicrichlet), we get the following linear system.

  $$ \begin{bmatrix} 
    2 & -1  &  0 & 0 & 0\\
    -1 & 2 & -1  &  0 & 0 \\
    0 & \ddots & \ddots & \ddots & 0 \\
    0 & 0 & -1 & 2 & -1 \\ 
    0 & 0 &  0 & -1 & 2 \\  
  \end{bmatrix}
  \begin{bmatrix}
    u(x_1) \\
    u(x_2) \\
    \vdots\\
    u(x_{n-1}) \\
    u(x_{n}) \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 +  h^2f(x_1) \\
    h^2f(x_2) \\
    \vdots\\
    h^2f(x_{n-1}) \\
    0 + h^2f(x_{n}) \\
  \end{bmatrix}$$  

  For case $(b)$, we impose a Neumann boundary condition that the unknown itself is
  not given at the right boundary so we have to solve for it. This
  means that our unknowns are not just at the interior points but also
  at any point where a Neumann condition is specified. Therefore, the
  size of the linear system is $m=n+1$, where $m$ is the total number
  of points and $n$ is the number of interior points.  The resulting
  linear system is
  $$ \begin{bmatrix} 
      2 & -1  &  0 & 0 & 0\\
    -1 & 2 & -1  &  0 & 0 \\
     0 & \ddots & \ddots & \ddots & 0 \\
     0 & 0 & -1 & 2 & -1 \\ 
     0 & 0 &  0 & -1 & 1 \\  
  \end{bmatrix}
  \begin{bmatrix}
    %  u(x_1) \\
    u(x_1) \\
    u(x_2) \\
    \vdots\\
    u(x_{n}) \\
    u(x_{n+1}) \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 +  h^2f(x_1) \\
    h^2f(x_2) \\
    \vdots\\
    h^2f(x_{n}) \\
    0 \\
  \end{bmatrix}$$  


  \paragraph{(b) Preconditioning based on sine transform}

  The preconditioner is $M = S D^{-1} S$, where $D=SAS$ (a similarity
  transformation) is a diagonal matrix~\cite{sine-transform}.  The
  diagonal matrix is inverted in $n$ operations, simply by finding the
  reciprocal of the diagonal elements. We exploit the properties of
  the sine-transform matrix $S$ that is is orthonormal (i.e. $S^T =
  S^{-1}$ and $SS^T = I$).

  \paragraph{(c) Comparison of CG and PCG}

  The CG and PCG algorithms were implemented in the computer code. The
  performances of each method for both test cases is summarized in
  Table~\ref{tab:summary}. It can be seen that the preconditioned algorithms
  perform much better than the CG algorithm. This is due to the nature
  of the preconditioning matrix $M$ is approximating the inverse using
  similarity transformation involing sine-transforms. The results are
  similar to the ones reported in the
  literature~\cite{sine-transform}.  An observation that can be made
  about the case $a$ is the vectors $S$ matrix are the actual
  eigenfunctions and therefore the preconditioner $M$ is the actual
  inverse. This leads to the theoretical convergence expected in one
  iteration. However, for case $b$, $M$ is not the actual inverse but
  indeed a close enough approximation which drastically reduced the
  number of iterations required to converge to the actual
  solution. This exercise emphasizes the importance of the choice of
  preconditioners. 

\begin{table}
  \caption{Comparison of number of iterations taken to converge with
    CG and PCG methods.}  \centering
  \begin{tabular}{c|c|c}
    \toprule
      n   &  Case $(a)$  & Case $(b)$ \\
      &  CG/PCG    & CG/PCG \\
      \midrule
      64    &  64/1   & 65/3  \\
      128   &  128/1   & 129/3  \\
      256   &  256/1   & 257/4 \\
      512   &  512/1 &  513/4\\
      1024  &  1024/1 &  1025/4\\
      \bottomrule
  \end{tabular}
  \label{tab:summary}
\end{table}


Figure~\ref{fig:profile} compares
  the solutions produced by each of these methods. All methods were
  converged to the same tolerance and therefore produce similar
  profile for the unknown $u(x)$. A minor discrepancy with the exact
  solution can also be noticed but it can be reduced with a higher
  tolerance for convergence.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{profile.pdf}    
    \caption{Plot of solution profile in the domain for different
      solution methods.}
    \label{fig:profile}
  \end{figure}

\begin{thebibliography}{9}
\bibitem{sine-transform} Chan, R. H., Ng, M. K., Wong,
  C. K. \emph{Sine transform based preconditioners for
    symmetric Toeplitz systems}. Linear Algebra and Its Applications,
  232, pp 237--259, 1996. doi:
  https://doi.org/10.1016/0024-3795(94)00049-2
\end{thebibliography}

  
\end{homeworkProblem}

\end{document}
